# 摘要
联合机器学习有望克服机器学习中的输入隐私挑战。

几个能够模拟联邦学习的项目的出现导致了该问题算法方面的相应快速进展。

然而，仍然缺乏联邦机器学习框架，这些框架专注于地理分布式环境中的可扩展性、健壮性、安全性和性能等基本方面。

为了弥补这一差距，我们设计并开发了 FEDn 框架。

FEDn 的一个主要特点是同时支持
跨设备和跨孤岛训练设置。

这使得 FEDn 成为在现实环境中研究各种机器学习应用的强大工具。

# Intro
总而言之，我们做出以下主要贡献：  
• 我们提出了一种系统架构和相关的编程模式，可确保跨孤岛和跨设备场景的高度可扩展、高效和健壮的联邦学习。  
• 我们提供了一个高效的开源框架实现，可以轻松地从本地测试和开发到生产级地理分布式部署，而无需更改代码。  
• 我们提供基于数千个（跨设备）地理分布的客户端和机器学习模型大小从几KB 到1GB（跨孤岛）的性能基准场景。  
据我们所知，本研究中提出的联合训练规模之前没有报道过。  

本文的其余部分安排如下。  
第 2 节给出了背景和调查相关工作。  
第 3 节解释了 FEDn 的架构和实现。  
在第 4 节中，我们描述了跨孤岛和跨设备用例的实验，并通过实际用例展示了框架的性能和可扩展性。  
最后，第 5 节总结了工作并概述了未来的方向。  

# Section 3 A flexible and horizontally scalable architecture for federated learning
## 3.1
![image](https://user-images.githubusercontent.com/65893273/142623408-0bdcf20b-000a-4bb2-b37b-580364c5fadc.png)

第二层由Combiners 和Reducer 协议组成。  
Combiner是无状态服务器，负责协调来自它们自己的客户端子集的更新。  
许多Combiner共同构成了 FEDn 网络。  

## 3.2
### 3.2.2 reducer
reducer 负责实现并执行一个 reducer 协议，将组合器计算的所有部分模型更新合并为每轮的单个全局模型更新。
在当前的实现中，reduce 操作被实现为一个单独的服务，它通过 gRPC 流从组合器中提取部分模型更新并连续聚合这些更新。
然而，一般而言，reducer 协议可以扩展为直接使用组合器或安全的多方计算协议来执行例如层次化reduce。
reducer 完成的工作随着组合器的数量（以依赖于协议的方式，在当前实现中呈线性）而缩放，并且与客户端的数量无关。
